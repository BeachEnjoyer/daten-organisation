---
# bibliography: ../../dxi-skript.bib

execute: 
  echo: false
---

# Informationstheorie {#sec-chapter-informationstheorie}

Die Arbeit mit Daten schliesst immer die Frage ein, ob diese Daten relevante Information verbergen. Dazu müssen wir zwei Konzepte erfassen:  

1. Daten 
2. Information

Beide Konzepte sind für unseren Alltag fast durchgehend von Bedeutung und ein intuitives Verständnis der beiden Konzepte sollte jedem vertraut sein. Beim intuitiven Umgang mit Daten und Information fällt auf, dass die beiden Begriffe häufig synonym gebraucht werden. Hier stellt sich ein erstes Problem: 

::: {.callout-warning}
## Problem

Wenn Daten und Information im Kern identisch sind, warum verwenden wir zwei Begriffe?
:::

Führen wir die beiden Begriffe auf ihren wörtlichen Ursprung zurück: 

- *Daten*: Pluralform von Datum, das sich auf das lateinische Verb *dare* zurückgeführt werden kann. *Dare* bedeutet *gegeben* und das von uns verwendete Substantiv bedeutet *das Gegebene* oder *die Gabe*. 
- *Information*: lässt sich ebenfalls auf das Lateinische zurückführen, wobei dieses Wort unverändert in unseren Wortschatz eingegangen ist. Die Bedeutung dieses Begriffs steht für *Auskunft* oder *Benachrichtigung*. 

Mit diesen beiden Ursprüngen können wir die Begriffe für Anwendungen vorläufig unterscheiden:

- *Daten* bezeichnen konkrete Werte.
- *Information* bezeichnet die *Bedeutung* der Daten. 

## Shannon's Informationsproblem

Claude Shannon befasste sich in den 1940er Jahren mit den Herausforderungen der (damals) modernen Kommunikationstechnologien Telegraphie und Telefon. Diese Technologien übertragen Nachrichten über einen Nachrichten*kanal*. Ein solcher Kanal kann ein Kabel oder auch eine Funkfrequenz sein. Dieser Nachrichtenkanal wird als *Medium* bezeichnet. 

::: {#def-kommunikation} 
Als *Kommunikation* wird das Übertragen von Informationen über ein Medium bezeichnet. 
:::

Diese analogen Technologien haben das Problem, dass sich Signale über längere Distanzen abschwächen. Dieser Effekt ergibt sich aus dem "Medium", dass für eine Kommunikation verwendet wird.  Ein Kabel hat z.B. eine Dämpfung, die mit der Länge des Kabels steigt. Je länger ein Kabel wird, desto grösser wird die Dämpfung. Die Dämpfung hat zur Folge, dass ein Signal leiser wird. Dadurch geht ein Teil des ursprünglichen Signals verloren. Dieser Prozess wird als "Equivocation" bezeichnet.

::: {.callout-note}
## Merke
Durch *Equivokation* gehen Informationen beim Übertragen verloren.
:::

Ein zweites Problem analoger Kommunikationstechnologien sind äussere Störungen des Mediums. Wird z.B. ein Magnet an ein Kabel gehalten, werden die über das Kabel übertragenen Signale verzerrt. Solche Veränderungen des Signals werden als Rauschen (engl. *Noise*) bezeichnet. Rauschen entsteht auch zufällig mit zunehmender Länge eines Mediums. 

::: {.callout-note}
## Merke
Durch Rauschen wird fehlerhafte Information den Daten hinzugefügt. 
:::

Shannon hat vor diesem Hintergrund die folgende Fragestellung untersucht:

::: {.callout-warning}
## Problem
Wie kann eine Nachricht ein Ziel erreichen, wenn die Daten durch Rauschen und Equivocation verändert werden? 
:::

@shannon_mathematical_1948 gliedert diese Problemstellung in Teilprobleme, indem der Kommunikationsprozess in Teilschritte gegliedert wird. Dabei ist die "geschickte" Gliederung von Bedeutung. Shannon hat den Kommunikationsprozess in sieben Komponenten unterteilt, indem er die bekannten Störungen der Nachrichtenübertragungen verbunden hat.

- Eine Informationsquelle, die Information erzeugt
- Kodieren der Information in eine Nachricht für ein Medium
- Das Übertragen der Nachricht über einen Kanal
- Das Empfangen und Dekodieren der Nachricht 
- Ein Informationsziel, die Information aufnimmt

Zusätzlich muss das *Rauschen* des Kanals berücksichtigt werden. Später wurde das Modell um die *Equivokation* erweitert [@shannon_communication_1949]. Sowohl das Rauschen als auch die Equivokation müssen eigenständig berücksichtigt werden, weil diese die Kommunikation unkontrolliert beeinflussen. Aus diese Überlegungen ergibt sich das Kommunikationschema in @fig-shannon-model.


```{mermaid}
%%| label: fig-shannon-model
%%| fig-cap: Integriertes Kommunikationsmodell [@shannon_mathematical_1948;@shannon_communication_1949]

flowchart LR
  a(Information) --> b(Kodieren)
  b --> c(Medium)
  c --> f(Dekodieren)
  
  c --> e(Equivokation)
  r(Rauschen) --> c

  f --> g(Information)

```

Shannon's besondere Leistung war, dass er diese Elemente als mathematische Funktionen über Wahrscheinlichkeiten (d.h. Werte zwischen `0` und `1`) formuliert und erkannt hat, dass Kommunikation dem Prinzip der **Entropie** folgt. Dieses Prinzip besagt, dass ein System mit einer niedrigen Entropie (d.h. einer geordneten Struktur) nur zu einer gleichbleibenden oder grösseren Entropie (d.h. zu mehr Unordnung) tendiert. Die Entropie vergrössert sich durch die Fehler bei der Kommunikation.

Daraus ergibt sich für Shannon's Theorie [@shannon_communication_1949] als direkte Konsequenz, dass Information und Daten über die folgende Funktion verbunden sind: 

$$
I(D) = P(D) - \epsilon
$$

- ``D`` steht dabei für die empfangenen Daten und
- $\epsilon$ ist die Summe der Wahrscheinlichkeiten aller Störungen im Kommunikationsprozess.

$$
\epsilon = P(S) + P(E) + P(N) + P(R) = \sum_{a} P(n_a)
$$

Mit 

- S = Senden/Kodieren
- E = Equivocation
- N = Rauschen (Noise)
- R = Empfangen/Dekodieren (Receiving)


Umgangssprachlich lassen sich diese Terme folgenderweise umschreiben: 

::: {#prp-information} 
Information ergibt sich aus Daten nachdem alle Fehler und Störungen in den Daten entfernt wurden.
:::

### Das Shannon Limit

Weil $P(D)$ ebenfalls eine Wahrscheinlichkeit ist, ergibt sich, dass Kommunikation nur dann möglich ist, solange die folgende Ungleichung gilt:

$$
P(D) \ge \epsilon
$$

Entsprechend wird $\epsilon$ in der Datenverarbeitung auch als **Shannon Limit** bezeichnet, weil dieser Term die absolute Grenze beschreibt, bis zu der eine fehlerfreie Kommunikation möglich ist. 

::: {.callout-note}
Diese Ungleichung besagt umgangssprachlich, dass Kommunikation nur möglich ist, solange weniger Fehler und Störungen als Daten übertragen werden. 
:::

Shannon hat nachgewiesen, dass jeder Kanal eine Grenze hat, ab der keine Datenübertragung mehr möglich ist.

## Informationstheorie in den Datenwissenschaften

Claude Shannon hat sich mit der technischen Kommunikation beschäftigt. Die Datenwissenschaften befassen sich mit dem Kodieren, dem Organisieren und dem Auswerten von Daten mit dem Ziel der Informationsgewinnung. Diese Schritte sind im Kern ein Kommunikationsprozess: Durch das Messen bestimmter Eigenschaften wird die Information der Realität als Daten erfasst. Anschliessend werden die gemessenen Daten zusammengefasst und strukturiert, damit sie abschliessend ausgewertet werden können. Das Messen entspricht dem Kodiere beim Senden, das Organisieren entspricht dem Daten*medium* und die Auswertung entspricht dem Dekodieren.

